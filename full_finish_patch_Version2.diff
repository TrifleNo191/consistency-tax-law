*** Begin Patch
*** Add File: CHANGELOG.md
+# Changelog
+
+## Unreleased / v0.1.0 - Full finish (2025-11-10)
+
+- Add full-finish experiment scaffolding and runnable artifact:
+  - Agent-based evolutionary simulation (experiments/abm).
+  - Sequence prediction toy task with baseline + ablations (experiments/sequence).
+  - Synthetic calibration experiment plotting and analysis improvements.
+  - Lightweight LLM contradiction emulator (experiments/llm) with hooks to replace with real model API.
+  - Orchestration script that runs all experiments and generates CSV + PNG artifacts.
+- Tests:
+  - Add tests for ABM and core CT utilities.
+  - Expand Hypothesis-based tests coverage.
+- CI:
+  - Update GitHub Actions to run typecheck, tests, lightweight experiments, and plotting; upload artifacts.
+- Docs:
+  - Add FULL_FINISH_README.md with step-by-step reproduction and next steps for heavier experiments.
+- Misc:
+  - Pin additional dependencies (matplotlib, tqdm).
+  - Add .gitignore entries for results and artifacts.
+
+Notes:
+- Experiments are designed to be fast and deterministic for CI; replacing the LLM emulator with a real model requires adding API keys and updating `experiments/llm/run_llm_contradiction.py`.
+
*** End Patch
*** Begin Patch
*** Add File: requirements.txt
+numpy==1.26.4
+pandas==2.2.2
+scipy==1.11.3
+matplotlib==3.8.1
+tqdm==4.66.1
+hypothesis==6.90.0
+pytest==8.4.3
+mypy==1.9.0
+black==24.1.0
+ruff==0.14.0
+
*** End Patch
*** Begin Patch
*** Add File: .gitignore
+# Virtual env
+.venv/
+venv/
+env/
+
+# Python
+__pycache__/
+*.pyc
+*.pyo
+*.pyd
+.pytest_cache/
+
+# outputs
+out/
+results/
+*.csv
+env.json
+*.png
+ci_out/
+
+# Editor
+.vscode/
+.idea/
+.DS_Store
+
*** End Patch
*** Begin Patch
*** Add File: experiments/synthetic/plot_ct_vs_calibration.py
+"""Plot CT vs calibration CSV produced by experiments/synthetic/run_ct_vs_calibration.py.
+
+Produces:
+ - outdir/ct_vs_calibration.png
+"""
+from __future__ import annotations
+import os
+import pandas as pd
+import matplotlib.pyplot as plt
+
+
+def plot(csv_path: str, outdir: str) -> str:
+    df = pd.read_csv(csv_path)
+    os.makedirs(outdir, exist_ok=True)
+    fig, ax1 = plt.subplots(figsize=(6, 4))
+    ax1.plot(df["c_uniform_mix"], df["D_KL"], label="D_KL(P||Q)", color="C0")
+    ax1.set_xlabel("Uniform mix coefficient c")
+    ax1.set_ylabel("D_KL (nats)", color="C0")
+    ax1.tick_params(axis="y", labelcolor="C0")
+
+    ax2 = ax1.twinx()
+    ax2.plot(df["c_uniform_mix"], df["CT_bound_J"], label="CT_bound_J", color="C1")
+    ax2.set_ylabel("CT bound (J)", color="C1")
+    ax2.tick_params(axis="y", labelcolor="C1")
+
+    fig.tight_layout()
+    png = os.path.join(outdir, "ct_vs_calibration.png")
+    plt.savefig(png, dpi=150)
+    plt.close(fig)
+    return png
+
+
+if __name__ == "__main__":
+    import argparse
+
+    p = argparse.ArgumentParser()
+    p.add_argument("--csv", required=False, default="out/synth/ct_vs_calibration.csv")
+    p.add_argument("--outdir", required=False, default="out/synth")
+    args = p.parse_args()
+    path = plot(args.csv, args.outdir)
+    print("Wrote", path)
+
*** End Patch
*** Begin Patch
*** Add File: experiments/abm/agent_evolution.py
+"""Lightweight agent-based evolutionary simulation used for the ABM experiment.
+
+This is intentionally simple and fast so CI can run a small parameter sweep.
+Outputs results CSV for analysis.
+"""
+from __future__ import annotations
+import os
+import numpy as np
+import pandas as pd
+from dataclasses import dataclass
+from typing import List
+
+
+@dataclass
+class Agent:
+    accuracy: float  # probability of correct world model
+    honesty: float   # probability of honest signaling
+    compartment: float  # ability to compartmentalize contradictions
+
+
+def step_environment(k: int = 5) -> np.ndarray:
+    rng = np.random.default_rng()
+    return rng.dirichlet(np.ones(k))
+
+
+def evaluate_agent(agent: Agent, P: np.ndarray, rng: np.random.Generator) -> float:
+    """Return resource acquired by agent in one timestep minus CT penalty."""
+    # Simulate agent's internal model draw correctness
+    correct = rng.random() < agent.accuracy
+    # Reward: overlap between internal guess and P
+    if correct:
+        reward = float(np.dot(P, P))
+    else:
+        # random guess penalty
+        q = rng.dirichlet(np.ones_like(P))
+        reward = float(np.dot(q, P))
+    # CT proxy: more divergence -> penalty, scaled by honesty and compartment
+    kl_model = 0.0 if correct else float(np.sum(P * np.log(np.clip(P / q, 1e-12, 1e12))))
+    kl_signal = 0.0 if rng.random() < agent.honesty else kl_model * 0.5
+    # compartment reduces CT impact
+    ct_penalty = (kl_model + kl_signal) * (1 - agent.compartment)
+    return reward - 0.1 * ct_penalty
+
+
+def run_sim(seed: int = 0, steps: int = 500, pop: int = 200, outdir: str = "out/abm") -> str:
+    rng = np.random.default_rng(seed)
+    os.makedirs(outdir, exist_ok=True)
+    # initialize population with random traits
+    agents: List[Agent] = [
+        Agent(accuracy=float(rng.random()), honesty=float(rng.random()), compartment=float(rng.random()))
+        for _ in range(pop)
+    ]
+    records = []
+    for t in range(steps):
+        P = step_environment(k=5)
+        fitness = []
+        for a in agents:
+            fitness.append(evaluate_agent(a, P, rng))
+        # selection: top 50% reproduce with mutation
+        idx = np.argsort(fitness)[::-1]
+        survivors = [agents[i] for i in idx[: max(1, pop // 2)]]
+        new_agents = []
+        for s in survivors:
+            # reproduce two children with small Gaussian mutation
+            for _ in range(2):
+                def mutate(x):
+                    y = x + rng.normal(0, 0.05)
+                    return float(np.clip(y, 0.0, 1.0))
+                child = Agent(mutate(s.accuracy), mutate(s.honesty), mutate(s.compartment))
+                new_agents.append(child)
+        agents = new_agents[:pop]
+        # log summary stats
+        records.append(
+            {
+                "t": t,
+                "mean_accuracy": float(np.mean([a.accuracy for a in agents])),
+                "mean_honesty": float(np.mean([a.honesty for a in agents])),
+                "mean_compartment": float(np.mean([a.compartment for a in agents])),
+            }
+        )
+    csv = os.path.join(outdir, "abm_summary.csv")
+    pd.DataFrame.from_records(records).to_csv(csv, index=False)
+    return csv
+
+
+if __name__ == "__main__":
+    import argparse
+
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--outdir", default="out/abm")
+    ap.add_argument("--seed", type=int, default=0)
+    ap.add_argument("--steps", type=int, default=200)
+    ap.add_argument("--pop", type=int, default=100)
+    args = ap.parse_args()
+    out = run_sim(seed=args.seed, steps=args.steps, pop=args.pop, outdir=args.outdir)
+    print("Wrote", out)
+
*** End Patch
*** Begin Patch
*** Add File: experiments/sequence/run_sequence_task.py
+"""Sequence-prediction toy task: baseline + two ablations.
+
+Baseline: model trained/initialized to match sequence statistics.
+Ablation 1: model miscalibrated (biased transition probabilities).
+Ablation 2: outputs corrupted (dishonest R).
+"""
+from __future__ import annotations
+import os
+import numpy as np
+import pandas as pd
+from typing import Tuple
+from ct.core import kl_divergence, compute_ct_bound
+from ct.config import CTConfig
+from ct.logging import set_seed, log_environment_json
+
+
+def make_markov_chain(k: int = 4, length: int = 200, rng: np.random.Generator = None) -> np.ndarray:
+    rng = rng or np.random.default_rng()
+    # random transition matrix
+    T = rng.dirichlet(np.ones(k), size=k)
+    states = [rng.integers(0, k)]
+    for _ in range(length - 1):
+        s = states[-1]
+        states.append(int(rng.choice(k, p=T[s])))
+    return np.array(states), T
+
+
+def empirical_distribution(states: np.ndarray, k: int) -> np.ndarray:
+    counts = np.bincount(states, minlength=k).astype(float)
+    return counts / counts.sum()
+
+
+def run(seed: int = 0, outdir: str = "out/sequence") -> str:
+    set_seed(seed)
+    rng = np.random.default_rng(seed)
+    os.makedirs(outdir, exist_ok=True)
+    states, T = make_markov_chain(k=4, length=500, rng=rng)
+    P = empirical_distribution(states, k=4)
+    # Baseline: Q approximates P (small smoothing)
+    Q_baseline = P.copy()
+    # Ablation 1: miscalibrated Q (biased towards state 0)
+    Q_miscalib = (1.0 - 0.3) * P + 0.3 * np.array([1.0, 0.0, 0.0, 0.0])
+    # Ablation 2: dishonest R fixed
+    R_dishonest = np.array([0.25, 0.25, 0.25, 0.25])
+
+    cfg = CTConfig(temperature=300.0, epsilon=1e-12, seed=seed)
+    res_base = compute_ct_bound(P, Q_baseline, temperature=cfg.temperature, eps=cfg.epsilon)
+    res_mis = compute_ct_bound(P, Q_miscalib, temperature=cfg.temperature, eps=cfg.epsilon)
+    res_dish = compute_ct_bound(P, Q_baseline, temperature=cfg.temperature, eps=cfg.epsilon)
+    df = pd.DataFrame(
+        [
+            {
+                "case": "baseline",
+                "Dkl_P_Q": float(kl_divergence(P, Q_baseline)),
+                "CT_J": float(res_base),
+            },
+            {
+                "case": "ablation_miscalib",
+                "Dkl_P_Q": float(kl_divergence(P, Q_miscalib)),
+                "CT_J": float(res_mis),
+            },
+            {
+                "case": "ablation_dishonestR",
+                "Dkl_P_Q": float(kl_divergence(P, Q_baseline)),
+                "CT_J": float(res_dish),
+            },
+        ]
+    )
+    out_csv = os.path.join(outdir, "sequence_baseline_ablation.csv")
+    df.to_csv(out_csv, index=False)
+    log_environment_json(os.path.join(outdir, "env.json"))
+    return out_csv
+
+
+if __name__ == "__main__":
+    import argparse
+
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--outdir", default="out/sequence")
+    ap.add_argument("--seed", type=int, default=0)
+    args = ap.parse_args()
+    p = run(seed=args.seed, outdir=args.outdir)
+    print("Wrote", p)
+
*** End Patch
*** Begin Patch
*** Add File: experiments/llm/run_llm_contradiction.py
+"""Lightweight LLM contradiction emulator.
+
+This script emulates an LLM's cost metrics under increasing contradiction counts.
+It is intentionally cheap: it uses a deterministic pseudo-model to generate
+a cost proxy (latency, compute_flops_estimate, attention_entropy).
+Replace the `emulator_loop` with a real model invocation when ready.
+"""
+from __future__ import annotations
+import os
+import numpy as np
+import pandas as pd
+from ct.logging import set_seed, log_environment_json
+
+
+def emulator_loop(contradiction_count: int, tokens: int, seed: int = 0, rng=None):
+    rng = rng or np.random.default_rng(seed)
+    # base latency (ms) and noise
+    base_latency = 5.0  # ms per token baseline
+    # emulate extra cost proportional to contradictions and token position
+    per_token_latency = base_latency + 0.5 * contradiction_count + 0.01 * np.log1p(np.arange(tokens))
+    # emulate attention entropy (proxy)
+    attention_entropy = 5.0 + 0.2 * contradiction_count + 0.05 * rng.normal(size=tokens)
+    # emulate flops proxy (arbitrary units)
+    flops = 1e6 + 1e5 * contradiction_count
+    return per_token_latency, attention_entropy, float(flops)
+
+
+def run(outdir: str = "out/llm", max_contradictions: int = 7, tokens: int = 512, seed: int = 0) -> str:
+    set_seed(seed)
+    os.makedirs(outdir, exist_ok=True)
+    rows = []
+    for c in range(0, max_contradictions + 1, 1):
+        latency, entropy, flops = emulator_loop(c, tokens, seed=seed)
+        rows.append(
+            {
+                "contradictions": int(c),
+                "mean_latency_ms": float(np.mean(latency)),
+                "median_latency_ms": float(np.median(latency)),
+                "mean_attention_entropy": float(np.mean(entropy)),
+                "flops_proxy": float(flops),
+            }
+        )
+    df = pd.DataFrame.from_records(rows)
+    csv = os.path.join(outdir, "llm_contradiction_metrics.csv")
+    df.to_csv(csv, index=False)
+    log_environment_json(os.path.join(outdir, "env.json"))
+    return csv
+
+
+if __name__ == "__main__":
+    import argparse
+
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--outdir", default="out/llm")
+    ap.add_argument("--tokens", type=int, default=512)
+    ap.add_argument("--max_contradictions", type=int, default=7)
+    ap.add_argument("--seed", type=int, default=0)
+    args = ap.parse_args()
+    out = run(outdir=args.outdir, max_contradictions=args.max_contradictions, tokens=args.tokens, seed=args.seed)
+    print("Wrote", out)
+
*** End Patch
*** Begin Patch
*** Add File: scripts/full_finish/run_all.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Run the lightweight full-finish experiment suite and produce plots
+python -m experiments.synthetic.run_ct_vs_calibration --outdir=out/synth --seed=0
+python experiments/synthetic/plot_ct_vs_calibration.py --csv out/synth/ct_vs_calibration.csv --outdir out/synth
+
+python -m experiments.sequence.run_sequence_task --outdir out/sequence --seed=0
+python -m experiments.llm.run_llm_contradiction --outdir out/llm --tokens=256 --max_contradictions=7 --seed=0
+python -m experiments.abm.agent_evolution --outdir out/abm --seed=0 --steps=200 --pop=100
+
+echo "Results written to out/; inspect CSVs and PNGs."
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/ci.yml
+name: CI
+on:
+  push:
+    branches: [ "main" ]
+  pull_request:
+
+jobs:
+  build-test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - uses: actions/setup-python@v5
+        with:
+          python-version: "3.11"
+      - name: Install deps
+        run: |
+          python -m pip install -U pip
+          pip install -r requirements.txt
+      - name: Type check
+        run: make typecheck
+      - name: Tests
+        run: make test
+      - name: Run lightweight experiments
+        run: |
+          make reproduce
+          bash scripts/full_finish/run_all.sh
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: ct-full-artifacts
+          path: out/
+
*** End Patch
*** Begin Patch
*** Add File: tests/test_abm.py
+from __future__ import annotations
+import os
+from experiments.abm.agent_evolution import run_sim
+import pandas as pd
+
+def test_abm_runs_and_outputs_csv(tmp_path):
+    outdir = str(tmp_path / "abm")
+    csv = run_sim(seed=1, steps=10, pop=20, outdir=outdir)
+    assert os.path.exists(csv)
+    df = pd.read_csv(csv)
+    assert "mean_accuracy" in df.columns
+    assert df.shape[0] == 10
+
*** End Patch
*** Begin Patch
*** Add File: docs/FULL_FINISH_README.md
+```markdown
+# Full-finish instructions â€” Consistency Tax (CT) repo
+
+Goal: run the full lightweight pipeline (fast CI-safe experiments), inspect artifacts, and know how to replace emulators with real experiments.
+
+Quickstart (local)
+1. Create venv and install pinned deps:
+   python -m venv .venv && source .venv/bin/activate
+   pip install -U pip
+   pip install -r requirements.txt
+
+2. Run tests:
+   make test
+
+3. Reproduce lightweight experiments + plots:
+   make reproduce
+   bash scripts/full_finish/run_all.sh
+
+Outputs:
+- out/synth/ct_vs_calibration.csv and ct_vs_calibration.png
+- out/sequence/sequence_baseline_ablation.csv
+- out/llm/llm_contradiction_metrics.csv
+- out/abm/abm_summary.csv
+
+How to move to heavy experiments (LLM real model)
+- Edit experiments/llm/run_llm_contradiction.py and replace `emulator_loop` with a real inference call.
+- Ensure API keys are read from environment variables (do not commit keys).
+- Update CI only after you have budgeted time/costs.
+
+Research gates (before claiming energetics)
+- Replace emulator with real LLM calls + measure wall-plug power via instrumentation.
+- Add bootstrap CIs and effect-size analyses for all primary metrics.
+- Provide full accounting of offloaded erasures or reversible bookkeeping.
+
+Contact & next steps
+I can prepare:
+- A Jupyter notebook that renders results and figure panels.
+- A PR description ready to post on GitHub.
+- Scripts to run GPU power profiling on a single host (nvidia-smi parsing).
+```
+
*** End Patch